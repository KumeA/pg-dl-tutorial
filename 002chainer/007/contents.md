# Chainerの基本：学習と最適化

ニューラルネットワークを含んだ多くの機械学習における学習タスクは最適なパラメータを探す問題に帰着することができます。

そして，最適なパラメータは目的関数の最小化（最大化）問題，つまり最適化問題を解くことで得られます。

一般に機械学習は次のステップからなります。

1) 学習対象のモデル F(\theta) を定義する

Chainerの場合，これはFunction, Link, Chainを組み合わせて入力から出力を求める関数を定義することに対応します。

2) 目的関数 L(F(\theta)) を定義する

Chainerの場合，多くの場合は損失関数（F.soft_cross_entropyや，F.mean_squared_error）に対応します。

また，分類の場合はL.Classifierを使います。

3) 目的関数を最小化するような\thetaを最適化問題を解くことで得る
   \theta^* = \arg \max L(F(\theta))

Chainerの場合，これは次のセッションで紹介するOptimizerを利用して解きます。

MEMO
======
目的関数が与えられた時，その最適解が解析的に解ける場合はそれを利用しますが
機械学習の多くの問題では最適解は解析的には解けない，または計算量がデータ数に対し二乗以上のオーダーで現実的に求められないので，勾配降下法を使って解きます。




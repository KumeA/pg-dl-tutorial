# 3. 目的関数を最適化することで，モデルを学習する

さて，これまで

1. 学習対象のモデルを定義する
2. 目的関数を定義する

の二つをみてきました。

ここまで，目的関数 $L(\theta)$ を最小化すれば学習を実現できるということをみてきました。

一般に関数 $F(x)$ について， $F(x)$ を最小化するような $x$ を求める問題を最適化問題とよびます。
例えば，1次元変数$x$について，

$$F(x)=x^2 + 4x - 7$$

という目的関数は

$$F(x)=(x+2)^2 - 11$$

であることから $x=-2$ が最小値を達成する変数であり，その時の最小値は $-11$ となります。

最適化問題は，パターゴルフのような問題とみなすことができます。
現在の変数 $x$ が位置に対応し，関数の値 $F(x)$ が高さに対応します．
目標は最も低い位置を探す問題です。

この問題を解くために，次のような戦略をとります。

1. 現在の位置 $x_t$ から目的関数の値が最も急激に下がりそうな方向を調べる

    * この方向を勾配と呼び，$-v_t$ と書きます。

2. その方向に向けて現在の位置から少し動かす: $x_{t+1} = x_t + \alpha_t v_t$

    * この時のステップ幅 $\alpha_t$ を更新律と呼びます。

このような最適化手法を勾配降下法と呼びます。

勾配降下法において，最も大変なのが勾配 $v_t$ の推定です。
実際のパターゴルフのような三次元の世界では，一番急激に下っている方向を探すのは簡単です。
一方，ディープラーニングにおいて，目的関数 $L(\theta)$ の変数 $\theta$ はパラメータの数だけ次元数があり，多い場合は数百万から数億に達します。

この勾配は，誤差逆伝播法（back propagation）を利用し推定します。

勾配は，あるパラメータを少しだけ動かした時に最終的な目標関数の値がどの程度変わるのかを調べることで求められます。

出力から入力に向かって，目的関数の出力についての勾配を伝播させていくことで効率的に勾配を求めることができます。

誤差逆伝搬法の計算量は順計算の計算量とほぼ同じであり，パラメータ数が多くても効率的に求めることができます。

Chainerはこの誤差逆伝播法をサポートし，任意の計算について誤差逆伝搬法を使って勾配を求めることができるようになっています。
`chainer.Variable` はこの誤差逆伝搬法を実現するために必要な情報を記録する仕掛けが入っています。

ユーザーが `Variable` を変数として順計算の計算手順を書いている時，Chainerは後で誤差逆伝播法ができるように内部で計算グラフを構築しています。

例えば，前回の `loss` を目的関数とした場合，途中のパラメータ，入力についての勾配は， `backprop` という関数を呼び出すことで求めることができます。

```
loss.backprop()
```

勾配情報はこの `loss` の計算に関わった全ての `Variable`, `Link` の `grad` 属性に格納されます。

さらに，学習対象のモデルのパラメータについての勾配を求め，その勾配情報に基づきパラメータを更新する手法が `chainer.optimizers` にサポートされています。

```
from chainer import optimizers
```

代表的な最適化手法はSGD, RMSprop, Adamなどです。

```
opt = optimizers.Adam()
opt.setup(model)
```

最適化エンジンがどの学習可能な関数を目標とするかは `setup` で設定します。
そして，勾配を求めて，その勾配情報を元に最適化します。

```
loss.backprop()
opt.update()
```

誤差逆伝搬法は強力で多くの関数の勾配を正確にかつ高速に求めることができます。

誤差逆伝播法で勾配が求められるので，これを使って目的関数の最適化ができますが，問題が一つあります。
目的関数 $L(\theta)$ の勾配 $v(\theta)$ は，目的関数の定義からデータ全てを調べないと求められません。
しかし，毎回勾配を求めるたびにデータを全て調べるのは計算コストが大きすぎます。
そのため，データ全体を使わずにデータの一部だけを利用し勾配の推定値を求め，
それを利用しパラメータを更新します。これを確率的勾配降下法（SGD）とよびます。

パターゴルフで例えるなら，大体こっちの方向が下ってそうだとわかったら，方向を細かく決めずにさっさと打ってしまう方法です。一回打つまでの時間が少なくてすむため単位時間あたりにより多くの回数打つことができ，真面目に方向を定めるより効率よく下ることができます。

この勾配推定に使うためにサンプリングされた学習データをミニバッチとよび，その個数をミニバッチサイズBとします。
ミニバッチサイズは例えば32〜1024程度の値が利用されます。

SGDは単に高速なだけではなく，よりよい最適化解を見つけることができます。
勾配降下法には次の二つの問題があります。
一つ目は最適解ではなく局所解に収束してしまう，つまり本当に一番小さい値ではないはその周辺からはみると小さい値に収束してしまう問題です。パターゴルフでいえば途中でくぼみがありそこにはまってしまうことに相当します。
二つ目はプラトーと呼ばれる平坦な領域にはまってしまう問題です。例えば平面のような場所では勾配が非常に小さく本当は平面の先にもっと良い解があったとしても，止まってしまう問題があります。

SGDは勾配の推定値を使って更新していくため，常に変数は推定によるノイズの影響である程度ランダムに動き続けます。
このランダムに動くことによって，局所解やプラトーから抜け出すことができます。

# 1. 学習対象のモデルを定義する

はじめに学習対象のモデルを定義します。

パラメータで特徴付けられるモデルをパラメトリックモデルとよび，本チュートリアルでは
パラメトリックモデルを対象にします。

パラメトリックモデルはパラメータ $\theta$ で特徴付けられた関数 $y=F(x; \theta)$ で表すことができます。
この関数 $F(x; \theta)$ は $x$ を受け取り， $y$ を返すような関数であり， $\theta$ によって挙動を変えます。

例えば，線形関数（またはアフィン変換）であるモデルは

f(x; θ) = Wx + b
θ = (W, b)

のように表すことができます。
この関数は，パラメータであるWやbを変えることでその挙動を変えることができます。

Chainerではこのようなパラメータがひも付いた関数をLinkとよびます。

ディープラーニングで利用される代表的なLinkは `chainer.links` でサポートされています。
また，自分で新しいLinkを作ることができます。

以降では，この `chainer.links` を `L` として使えるようにします。

```
from chainer import links as L
```

例えば，先程の線形変換は `Linear` という名前のLinkとして用意されており，例えば入力が100次元，
出力が20次元の線形変換を表す `Linear` は次のように作ることができます。

```
lin = L.Linear(100, 20)
```

この `lin` はLinkオブジェクトですが，次のように関数呼び出しをすることができます。
（この関数呼び出しは `__call__` で定義されおり，演算子オーバーロードで実現されています。）

```
import numpy as np
from chainer import Variable
...
x = Variable(np.ones((10, 100)), dtype=np.float32)
y1 = lin(x)
```

この `numpy` ， `Variable` については後で詳しく説明します。
ここでは， `np.ones((10, 100))` は10行100列で全ての値が1であるような行列を作り
`Variable` は値に加えて学習に必要な情報が埋め込まれているオブジェクトとだけ覚えてください。
ここでは，10個の100次元のベクトルを用意し，それをVariableというオブジェクトにセットし，
それを `lin` の引数といて与えて，出力を `y` として計算しています。

`Variable v` に格納されている値は `v.data` として参照できます。
例えば，上の例の入力と出力は

```
print(x.data)
print(y1.data)
```

で調べることができます。

また，パラメータで特徴づけられていない関数はFunctionとよびます。
ディープラーニングで利用されている代表的な関数は `chainer.functions` で定義されています。
また，自分で新しいFunctionを作ることができます。

以降では，この  `chainer.functions` をFとして使えるようにします。
```
from chainer import functions as F
```

例えば，ディープラーニングでよく使われるReLUとよばれる非線形関数 `f_{relu}` は

$f_{relu}(x)=max(x,0)$

```
from chainer import functions as F
...
y2 = F.relu(x)
```

これらのLinkとFunctionを組みわせて複雑な関数を作ることができます。
例えば，線形変換を適用した後にReLUを適用した結果は次のように計算されます。

```
y3 = F.relu(lin(x))
```

## Softmax

d次元の実数値ベクトルから，d次元の確率分布を作る方法として，Softmax（または他クラスロジスティックスモデル）が知られています。

$Softmax(x)$ は，$x$ が $d$ 次元であり，各次元の値が $x[0], x[1], ..., x[d-1]$ の時，
$y[i]=exp(x[i])/\sum_i exp(x[i])$ と表されます。

あるベクトルvが確率分布となる条件として，

1. 各次元の値が非負
2. 合計値が1

という条件があります。
Softmaxは， $exp$ が非負であることから1の条件を満たし，各次元の値を足した値で割っていることから2の条件を満たします。

```
y = F.softmax(x)
```

## 課題

100行10列のランダムな値で埋められた行列に対して，上の例の `lin` を適用した後に ReLU を適用し，その結果を表示しなさい。
なお，100行10列のランダムな値は `np.random((100, 10))` で作ることができます。

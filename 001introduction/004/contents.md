# 1. 学習対象のモデルを定義する

はじめに学習対象のモデルを定義します。

パラメータで特徴付けられるモデルをパラメトリックモデルとよび，本チュートリアルでは
パラメトリックモデルを対象にします。

パラメトリックモデルはパラメータθで特徴付けられた関数y=F(x; θ)で表すことができます。
この関数F(x; θ)はxを受け取り，yを返すような関数であり，θによって挙動を変えます。

例えば，線形関数（またはアフィン変換）であるモデルは

f(x; θ) = Wx + b
θ = (W, b)

のように表すことができます。
この関数は，パラメータであるWやbを変えることでその挙動を変えることができます。

Chainerではこのようなパラメータがひも付いた関数をLinkとよびます。

ディープラーニングで利用される代表的なLinkはchainer.linksにサポートされています。
また，自分で新しいLinkを作ることができます。

以降では，このlinksをLとして使えるようにします。

```
from chainer import links as L
```

例えば，先程の線形変換はLinearという名前のLinkとして用意されており，例えば入力が100次元，
出力が20次元の線形変換を表すLinearは次のように作ることができます。

```
lin = L.Linear(100, 20)
```

このlinはLinkオブジェクトですが，次のように関数呼び出しをすることができます。
（この関数呼び出しは__call__で定義されおり，演算子オーバーロードで実現されています。）

```
import numpy as np
from chainer import Variable
...
x = Variable(np.ones((10, 100)), dtype=np.float32)
y1 = lin(x)
```

このnumpy，Variableについては後で詳しく説明します。
ここでは，np.ones((10, 100))は10行100列で全ての値が1であるような行列を作り
Variableは値に加えて学習に必要な情報が埋め込まれているオブジェクトとだけ覚えてください。
ここでは，10個の100次元のベクトルを用意し，それをVariableというオブジェクトにセットし，
それをlinの引数といて与えて，出力をyとして計算しています。

Variable vに格納されている値はv.dataとして参照できます。
例えば，上の例の入力と出力は

```
print(x.data)
print(y1.data)
```

で調べることができます。

また，パラメータで特徴づけられていない関数はFunctionとよびます。
ディープラーニングで利用されている代表的な関数はchainer.functionsで定義されています。
また，自分で新しいFunctionsを作ることができます。


以降では，このfunctionsをFとして使えるようにします。
```
from chainer import functions as F
```

例えば，ディープラーニングでよく使われるReluとよばれる非線形関数f_{relu}は

f_{relu}(x) = max(x, 0)



```
from chainer import functions as F
...
y2 = F.relu(x)
```

これらのLinkとFunctionを組みわせて複雑な関数を作ることができます。
例えば，線形変換を適用した後にReluを適用した結果は次のように計算されます。

```
y3 = F.relu(lin(x))
```



## Softmax

d次元の実数値ベクトルから，d次元の確率分布を作る方法として，Softmax（または他クラスロジスティックスモデル）が知られています。


Softmax(x)は，xがd次元であり，各次元の値がx[0], x[1], ..., x[d-1]の時，
y[i] = exp(x[i]) / \sum_i exp(x[i])

あるベクトルvが確率分布となる条件として，
1) 各次元の値が非負
2) 合計値が1
という条件があります。
Softmaxは，expが非負であることから1)の条件を満たし，各次元の値を足した値で割っていることから2)の条件を満たします。


```
y = F.softmax(x)
```

### 課題

100行10列のランダムな値で埋められた行列に対して，上の例のlinを適用した後にReluを適用し，その結果を表示しなさい。
なお，100行10列のランダムな値はnp.random((100, 10))で作ることができます。



